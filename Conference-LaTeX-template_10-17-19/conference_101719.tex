\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Sistemi e Architetture per Big Data - AA 2020/2021 \\
	\LARGE \emph{Primo progetto}}

\author{\IEEEauthorblockN{Giuseppe Lasco}
\IEEEauthorblockA{\textit{Dipartimento di Ingegneria dell'Informazione} \\
\textit{Universit\`{a} degli studi di Roma "Tor Vergata"
}\\
Roma, Italia \\
giuseppe.lasco17@gmail.com}
\and
\IEEEauthorblockN{Marco Marcucci}
\IEEEauthorblockA{\textit{Dipartimento di Ingegneria dell'Informazione} \\
\textit{Universit\`{a} degli studi di Roma "Tor Vergata"
}\\
Roma, Italia \\
marco.marcucci96@gmail.com}

}

\maketitle

\begin{abstract}
Questo documento riporta i dettagli implementativi
riguardanti l'analisi mediante \emph{Spark} dei dataset contenenti
informazioni relative all'andamento nazionale italiano dei
vaccini effettuati. Viene, inoltre, descritta l'architettura 
a supporto dell'analisi e gli
ulteriori ​\emph{framework}​ utilizzati.
\end{abstract}

\section{\textbf{Introduzione}}
L'analisi effettuata si pone lo scopo di valutare delle
statistiche relative ai vaccini contro il COVID-19, su dati 
resi disponibili dal Commissario straordinario per l'emergenza Covid-19, 
Presidenza del Consiglio dei Ministri.\\
\subsection*{\textbf{Dataset}} 
Il primo file preso in considerazione \`{e} \textit{punti-somministrazione-tipologia.csv}, il quale contiene dati sui punti di somministrazione per ciascuna Regione e Provincia Autonoma.
\par Il secondo file preso in considerazione \`{e} \textit{somministrazioni-vaccini-latest.csv}, il quale contiene dati sulle somministrazioni giornaliere dei vaccini suddivisi per regioni, fasce d'et\`{a} e categorie di appartenenza dei soggetti vaccinati. Tale dataset risulta ordinato per data,
inoltre \`{e} stata riscontrata l'assenza di numerose tuple relative a delle specifiche regioni, fasce d'et\`{a} e mesi. Questo fenomeno ha reso necessario un intervento di preprocessamento utile a inserire date mancanti per rendere pi\`{u} accurato il lavoro di regressione sui dati, sotto l'assunzione che i dati mancanti fossero dovuti all'assenza di vaccinazioni in un determinato giorno. 
\par Il terzo file preso in considerazione \`{e} \textit{somministrazioni-vaccini-summary-latest.csv}, il quale contiene dati sul totale delle somministrazioni giornaliere per regioni e categorie di appartenenza dei soggetti vaccinati. Il dataset in questione risulta, invece, non ordinato, per cui
si \`{e} reso necessario un effort di preprocessamento al fine di ordinarlo. 
\par L'ultimo file preso in considerazione \`{e} \textit{totale-popolazione.csv}, che tiene traccia
della popolazione totale residente in una data Regione o Provincia Autonoma.
\subsection*{\textbf{Query}}
L'obiettivo di questo progetto \`{e} quello di implementare ed eseguire tre query utilizzando \emph{Spark}.
\par La prima query ha come scopo quello di calcolare il numero medio di vaccinazioni giornaliere
in ciascun centro di ciascuna area.
\par La seconda consiste nel determinare le prime 5 aree per le quali è previsto il maggior numero di
vaccinazioni il primo giorno del mese successivo per le donne, per ogni fascia anagrafica e per ogni mese solare. A tale scopo si utilizza una retta di regressione, addestrata sui dati relativi al mese
precedente a quello per cui viene fatta la predizione al primo giorno. I dati presi in considerazione partono dal 1 Febbraio 2021.
\par L'ultima query prevede di effettuare una previsione della percentuale totale delle somministrazioni dei vaccini al 1 Giugno 2021 per ogni regione, utilizzando tutti i dati relativi ai mesi precedenti, a partire dal 27 Dicembre 2020. Inoltre, vengono utilizzati due algoritmi di
clustering in grado di raggruppare le Regioni in base alla previsione sopra citata.
\subsection*{\textbf{Framework}}
Il progetto prevede l'utilizzo di alcuni \emph{framework} che permettono di rendere la computazione parallela e distribuita.
Come \emph{framework} di processamento batch \`{e} stato utilizzato \emph{Apache Spark} che comunica con
 lo storage distribuito \emph{Hadoop Distributed File System}. Per la raccolta dei risultati \`{e} stato impiegato \emph{HBase}, uno storage No-SQL column family. Infine, come \emph{framework} di data ingestion \`{e} stato utilizzato \emph{NiFi}.
\section{\textbf{Architettura}}
L'architettura si compone di un insieme di container \emph{Docker}, su cui eseguono i servizi introdotti precedentemente. Inoltre, sempre sulla stessa macchina, una JVM ospita l'esecuzione di \emph{Apache Spark}. I container comunicano attraverso la stessa rete, creata appositamente.
\begin{figure}[htbp]
\includegraphics[scale=0.24]{frameworks.png}
\caption{Schema dell'architettura}\label{figura:architettura}
\label{fig}
\end{figure}

\subsection*{\textbf{NiFi}}
\emph{NiFi} \`{e} il servizio che permette di recuperare i dataset in formato \emph{comma separated value} da \emph{GitHub}, trasformarli in formato \emph{parquet} e inviarli al servizio di storage distribuito \emph{HDFS}. Tale \emph{framework} \`{e} stato istanziato su container \emph{Docker} utilizzando l'immagine \emph{apache/nifi}.
L'uso di \emph{parquet} ha permesso di comprimere i dati migliorando le prestazioni in termini di occupazione di memoria. \textbf{***colonne***}
\par Al fine di eseguire le operazioni elencate, sono stati impiegati due \emph{processori}, uno che permette di collegarsi al servizio di hosting \emph{GitHub} e scaricare i dati e uno che permette la trasformazione in \emph{parquet} di questi ultimi e l'upload su \emph{HDFS}. La struttura \`{e} definita mediante il template in figura ~\ref{figura:template}.
\begin{figure}[htbp]
\includegraphics[scale=0.19]{NiFi_template.png}
\caption{NiFi template}\label{figura:template}
\label{fig}
\end{figure}

\subsection*{\textbf{HDFS}}
\emph{HDFS} rappresenta il mezzo che permette l'archiviazione dei dati in maniera distribuita. Il servizio si compone di un nodo \emph{master} e tre nodi \emph{worker} con un livello di replicazione pari a 2. Tale servizio permette di rendere disponibili i dati su cui \emph{Spark} esegue la computazione e memorizza gli \emph{output} dell'analisi, che vengono, in seguito, esportati su \emph{HBase}, per eventuali analisi, manipolazione e rappresentazione dei dati. Il deployment del framework avviene attraverso l'utilizzo dell'immagine \emph{Docker} \emph{effeerre/hadoop}, istanziata su container. In seguito all'avvio del servizio, uno script permette di eseguire lo sturtup del \emph{Namenode} e dei \emph{Datanode}, e crea le directory \texttt{/data}, dove \emph{NiFi} inserisce i dati, e \texttt{/output}, in cui risiedono i risultati dell'analisi, concedendo i permessi di lettura, scrittura ed esecuzione.

\subsection*{\textbf{Spark}}
Al fine di preocessare i dati ed eseguire le \emph{query}, viene utilizzato \emph{Apache Spark} in locale, tramite lo script \texttt{\$SPARK\_HOME/bin/spark-submit}. Oltre allo \emph{Spark Core}, che espone un set di API di \emph{trasformazioni} ed \emph{azioni}, \`{e} stata impiegata la libreria di \emph{Machine Learning} \emph{MLlib}, utile per effettuare \emph{clustering} sui risultati della terza \emph{query}. 

\subsection*{\textbf{HBase}}
Hbase \`{e} stato utilizzato come datastore \emph{NoSQL} sul quale
importare i risultati delle ​query,​ anche questo servizio \`{e}
stato istanziato utilizzando un ​container \emph{Docker} realizzato,
questa volta, a partire dall'immagine​ \emph{harisekhon/hbase}​. Affinch\`{e} fosse possibile
l'esportazione dei risultati da \emph{HDFS} a \emph{HBase}, \`{e} stata creata la classe \texttt{HBaseQueries.java} che permette la creazione delle tabelle e l'inserimento dei dati, sfruttando la classe \texttt{HBaseClient.java}. Quest'ultima contiene informazioni riguardo la configurazione di \emph{HBase} e \emph{Zookeeper}, e le principali operazioni di gestione del datastore.\\
\section{\textbf{Query}}
\subsection*{\textbf{Query 1}}
Al fine di soddisfare la seguente \emph{query}, si \`{e} reso necessario l'utilizzo di due file, \textit{somministrazioni-vaccini-summary-latest.parquet} e \textit{punti-somministrazione-tipologia.parquet}.\par
Tali file sono stati caricati in \texttt{Dataset} e trasformati in \texttt{JavaPairRDD}, considerando le sole colonne di interesse: \emph{data\_somministrazione}, \emph{area} e \emph{totale} per il primo e \emph{area} per il secondo.\par \`{E} stato effettuato un'ordinamento dell'\texttt{RDD} \textit{somministrazioni-vaccini-summary-latest} in base alla data, scartando i dati precedenti al 1 Gennaio 2021 e successivi al 31 Maggio 2021, utilizzando la \emph{trasformazione} di \emph{filter}. Un'\emph{azione} di \emph{reduceByKey} ha permesso di ottenere il totale di vaccinazioni per ogni mese. Utilizzando un approccio simile al \emph{word count}, sono stati contati i centri riferiti ad una determinata Regione relativi all'\texttt{RDD} \textit{punti-somministrazione-tipologia}. La \emph{join} ha permesso di unire i due \texttt{RDD}, utilizzando come chiave la Regione. Il risultato finale \`{e} stato ottenuto dividendo il totale per il numero di giorni del mese di riferimento e per il numero di centri della regione di riferimento, ordinando, infine, il risultato in termini di mese e regione.

\subsection*{\textbf{Query 2}}
Relativamente alla seconda \emph{query} \`{e} stato utilizzato il file \textit{somministrazioni-vaccini-latest.parquet}, il quale, in seguito al caricamento da \emph{HDFS}, \`{e} stato trasformato in \texttt{JavaPairRDD}. Durante questa fase sono state scartate le colonne irrilevanti ai fini della richiesta. La \emph{trasformazione "filter"} ha permesso l'eliminazione delle entry relative a date precedenti al 1 Febbraio 2021 e successivi al 1 Giugno 2021. Considerando come chiave la tupla \emph{area, data e fascia anagrafica} sono stati sommati i vaccini relativi ad aziende farmaceutiche differenti. La \emph{trasformazione "groupByKey"} \`{e} stata applicata al fine di raggruppare tutte le tuple \emph{data, numero somministrazioni giornaliere} relative ad una certa regione e fascia anagrafica. Per ogni mese \`{e} stato eseguito una operazione di inserimento di date e valori macanti ed \`{e} stata effettuata \emph{regressione lineare} per ogni mese, in modo da prevedere il numero di donne vaccinate al primo giorno del mese successivo. Il modello di regressione lineare \`{e} stato addestrato attraverso l'implementazione fornita dalla libreria di regressione di \texttt{Apache Commons}.

\subsection*{\textbf{Query 3}}
L'ultima \emph{query} fa uso dei dati presenti nei file \textit{somministrazioni-vaccini-summary-latest.parquet} e \textit{totale-popolazione.parquet}. In seguito si \`{e} passati al caricamento dei file e alla trasformazione in \texttt{JavaPairRDD}. Sui dati relativi a \textit{somministrazioni-vaccini-summary-latest} si \`{e} proceduto al raggruppamento delle tuple \emph{data, numero somministrazioni giornaliere} per ogni regione, questa operazione ha permesso di svolgere regressione lineare su tutti i giorni dal 27 Dicembre 2020 al 31 Maggio 2021, in modo da prevedere il numero di vaccini effettuati in data 1 Giugno 2021. Una \emph{reduceByKey} sulle regioni ha, invece, permesso di calcolare il totale di vaccini effettuati dal 27 Dicembre 2020 al 31 Maggio 2021. Infine, l'operazione di somma tra le proiezioni e il totale calcolato, ha decretato il numero totale previsto di vaccinati per regione al 1 Giugno 2021. Il \emph{join} tra l'\emph{RDD} in questione e quello contenente il numero totale di abitanti residenti in ciacuna regione, he reso possibile clcolare la percentuale prevista di vaccinati al 1 Giugno 2021. Utilizzando due algoritmi presenti in \emph{MLLib}, \`{e} stato effettuato \emph{clustering} utilizzando i risultati precedenti come dataset, con numero di cluster variabile da 2 a 5. Gli algoritmi utilizzati sono \emph{K-means} e \emph{Bisecting K-means}, mentre per la regressione \`{e} stata utilizzata l'implementazione fornita dalla libreria di regressione di \texttt{Apache Commons}.

\section{\textbf{Benchmark}}
L'esecuzione del progetto e la valutazione delle prestazioni sono state eseguite su Linux Mint 19.3 Cinnamon, Intel Core i7-8750H CPU, 6 core, 12 thread e 32 GB di RAM, con archiviazione su HDD. \\
\par In tabella~\ref{tab1} sono riportati i tempi di processamento delle query. Sono state considerate le performance al netto di sturtup della \emph{Java Virtual Machine} su cui \emph{Spark} opera e caricamento dei file dall'\emph{HDFS}. I tempi scrittura dei risultati sul \emph{file system distribuito}, invece, sono stati considerati, in modo da rendere effettive tutte le operazioni (trasformazioni) eseguite da \emph{Spark}. Come si pu\`{o} notare, la query 3 risulta molto pi\`{u} lenta delle altre due, che, invece, mostrano risultati migliori. Tale evidenza \`{e} causata dall'inclusione, nel totale, dei tempi di addestramento degli algoritmi di \emph{clustering}, che possono essere osservati in tabella~\ref{tab2}.
\par Sempre dalla tabella~\ref{tab2} \`{e} possibile notare che i tempi di addestramento del primo modello (\emph{K-Means con numero di cluster pari a 2}) risultano superiori, a quelli relativi alle altre combinazioni, per via dei tempi di startup della libreria di \emph{clustering} di \emph{Spark}.
\par In tabella~\ref{tab3}, invece, \`{e} possibile osservare come il \emph{Within Set Sum of Squared Error} medio, per i due algoritmi di \emph{clustering}, decresca al crescere del numero di cluster, anche se questo non \`{e} scontato per via dell'inizializzazione randomica dei centroidi. 

\begin{table}[htbp]
\caption{Tempi esecuzione query}
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Query} & \textbf{Media} & \textbf{Varianza}  \\ \hline
    Query 1 & 221.1 & 41.1  \\ \hline
    Query 2 & 925.2 & 34.2 \\ \hline
    Query 3 & 2778.4 & 260.3 \\ \hline
    \multicolumn{3}{l}{$^{\mathrm{*}}$I tempi sono espressi in millisecondi.}
    \end{tabular}
\label{tab1}
\end{center}
\end{table}


\begin{table}[htbp]
\caption{Tempi esecuzione clustering}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{}&\multicolumn{4}{|c|}{\textbf{Modello}} \\ \hline
\textbf{Numero} & \multicolumn{2}{|c|}{\textbf{K-means}} & \multicolumn{2}{|c|}{\textbf{Bisecting K-means}} \\ \cline{2-5} 
\textbf{cluster} & \textbf{Media} & \textbf{Varianza} & \textbf{Media} & \textbf{Varianza} \\ \hline
2 & 347.3 & 43.6 & 203.2 & 62.4 \\ \hline
3 & 136.2 & 21.6 & 127.1 & 24.8 \\ \hline
4 & 153.1 & 39.5 & 148.2 & 25.5 \\ \hline
5 & 148.9 & 51.9 & 144.3 & 42.1 \\ \hline
\multicolumn{5}{l}{$^{\mathrm{*}}$I tempi sono espressi in millisecondi.}
\end{tabular}
\label{tab2}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Costo clustering: WSSSE}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Numero}&\multicolumn{2}{|c|}{\textbf{Modello}} \\ \cline{2-3}
\textbf{cluster} & \multicolumn{1}{|c|}{\textbf{K-means}} & \multicolumn{1}{|c|}{\textbf{Bisecting K-means}} \\ \hline
2 & 0.005392 & 0.005639 \\ \hline
3 & 0.003129 & 0.002325 \\ \hline
4 & 0.001469 & 0.001585 \\ \hline
5 & 0.000751 & 0.000751 \\ \hline
\end{tabular}
\label{tab3}
\end{center}
\end{table}

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}

\end{document}
